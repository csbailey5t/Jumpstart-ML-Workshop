{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9ZZbCVgPIun"
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "### Structure\n",
    "\n",
    "- Conceptual introduction to machine learning\n",
    "- Hands-on document classification as an example machine learning workflow\n",
    "- Library applications of machine learning\n",
    "- Be aware of existing resources to further your understanding machine learning and its applications.\n",
    "\n",
    "## Some Definitions\n",
    "### Data Science and its Subfields\n",
    "**Data science** is an all encompassing process of collecting, managing, processing, analyzing, and visualizing and reporting the inferences made on data. The following graphics from UC Berkley depicts the data science cycle in five stage, along with some of the techniques associated with each stage.\n",
    "\n",
    "<img\n",
    "src=\"https://github.com/vrtompki/jumpstart/blob/master/Images/Data Science Process.png?raw=1\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Aritificial intelligence** (AI) describes the ability of a computer to performed learned tasks. **Machine learning** (ML) is the process of training a machine or computer to perform a task. In the previous image, ML is tool used primarily during, but not restricted to, the \"Process\" and \"Analyze\" portion of the data science cycle. **Deep learning** (DL) refers to an area of machine learning focused on using large, multi-layered neural networks. The depth of a neural network depends on how many hidden layers the network has. Deep learning is a subsection of machine learning, and machine learning is a subset of artificial intelligence, and all three areas are tools that can be used by data scientists to understand and communicate the analysis of data.\n",
    "\n",
    "<img\n",
    "src=\"https://github.com/vrtompki/jumpstart/blob/master/Images/AI_ML_DL.png?raw=1\"/>\n",
    "\n",
    "### Machine Learning from a Distance\n",
    "Teaching or training a computer or machine to perform a task is very similar to how humans learn throughout their lifetime. Bob Ross was a famous jovial painter who was known for his television program that tried to teach viewers to paint amazing scenes from nature that included his signature \"happy trees\". He would often mix colors to add to his palette to get various shades of green to paint the trees. Given only the primary colors, how would one replicate Bob's technique to create a nice shade of green for their \"happy tree\"?\n",
    "\n",
    "<p float=\"left\"><img src=\"https://github.com/vrtompki/Linear_Regression_Tutorial/blob/master/images/bob_ross.jpg?raw=1\" style=\"width: 200px;\"/>\n",
    "    <img src=\"https://github.com/vrtompki/Linear_Regression_Tutorial/blob/master/images/primary_colors.png?raw=1\" style=\"width: 400px;\"/>\n",
    "</p>\n",
    "\n",
    "Well you may remember that green is a combination of the colors *yellow* and *blue*, and you may notice that these two colors were already provided. You try mixing different amounts of these colors until you are finally able to create the three desired shades of *green*!\n",
    "\n",
    "<img src=\"https://github.com/vrtompki/Linear_Regression_Tutorial/blob/master/images/color_mixing_example.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Different shades of green consists of a various amounts of two primary colors, *yellow* and *blue*.  So theoretically we could represent any shade of green using a combination of the two colors:\n",
    "\n",
    "$$ Green = Yellow*amount_{yellow} + Blue*amount_{blue}$$\n",
    "\n",
    "$$ \\begin{align*}\n",
    "Green_{1} &= Yellow*1 + Blue*2 \\\\\n",
    "Green_{2} &= Yellow*2 + Blue*1 \\\\\n",
    "Green_{3} &= Green_{1} + Green_{2} + Blue*1 \\\\\n",
    "\\end{align*} $$\n",
    "\n",
    "When we train a machine, we are trying to find the best parameters of the equations to complete the task. In the previous example, albeit non-practical, we would train the model to predict the shade of green and teh amount of blue and yellow required to make it.  One of the simplest examples is the use of a straight line to predict an output similar to the equations above which represents the sum of two lines. For simplicity if we look at a single line equation, it contains a slope (the rate of change of the line, or the degree of incline) and an intercept (the base component that needs to be added to all values, usually an indicator of inherent noise of the data). By finding the slope and the intercept of the line, we can then use it to map the inputs to specific output values. This particular machine learning task is called regression. Next we will go over some fo the tasks and the types of data used for machine learning.\n",
    "\n",
    "## Types of Machine Learning Tasks\n",
    "### Supervised vs Unsupervised\n",
    "Tasks are performed in one of three manners:\n",
    "- **Supervised** - the input data is labeled, or samples with know classes are used to train the model.\n",
    "- **Unsupervised** - class membership of the input data is not known, or is not important.\n",
    "- **Semi-supervised** - uses both supervised and unsupervised techniques for analyzing data.\n",
    "\n",
    "### Classification vs Predicition\n",
    "All machine learning models are designed to do one of two tasks **classification** or **prediction**. Classifiers are models trained to identify inputs as belonging to a specific group of objects (i.e. determining if an image is a cat or a dog). \n",
    "\n",
    "<img src=\"https://github.com/vrtompki/jumpstart/blob/master/Images/Machine Learning.png?raw=1\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJFYI-Q6Pdjg"
   },
   "source": [
    "## The What, Why, and How of machine learning - a conceptual introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCDD3SgdlPHd"
   },
   "source": [
    "<figure>\n",
    "  <img  src=\"http://scikit-learn.org/stable/_static/ml_map.png\" width=\"75%\" />  <figcaption><div align=\"left\" style=\"padding-top: 4px;\">Source: <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\">`scikit-learn`: Choosing the right estimator</a></div></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wwe66-4YS6ou"
   },
   "source": [
    "## Classifying textual documents - an example ml workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HX2iLwVN08Mo"
   },
   "source": [
    "### A typical ML classification workflow\n",
    "Generally, the workflow for any classification task is usually as follows: \n",
    "\n",
    "1. **Collect** or create **labeled data**\n",
    "2. **Transform** that data into a numeric representation\n",
    "  - Each numeric value representing a characteristic of the data is called a **feature**\n",
    "  - The set of all features representing a single pair of input data and labels is called the **feature vector**\n",
    "  - The whole labeled data set is split into two parts (at least) to train, evaluate and refine the model: a **training set** and a **test set**\n",
    "3. **Train** (learn/fit) a model on a part of the transformed labeled data (the training set)\n",
    "4. **Test** the model predictions on the test set to evaluate its performance\n",
    "5. **Assess** your model and revisit each of the previous steps, if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fC5UUkFllPHe"
   },
   "source": [
    "#### Text classification\n",
    "\n",
    "\n",
    "Given the specific task of assigning a category to a new text based on a set of labeled input texts:\n",
    "\n",
    "1. **Collect and label.** In the case of Twitter data, for example, you'd download a bunch of tweets using the Twitter API, extract their texts from the JSON format of the API response, and then assign one or more labels to each tweet (the easiest way is just to use the tweet's hashtags as labels).\n",
    "2. **Transform.** There are many strategies for turning your textual data into numbers, and `scikit-learn` has built-in libraries for most of them. Usually you'll begin by making a list of the words that apear in a document (a \"bag of words\"), but probably you'll also want to count the *frequencies* of the words in each document (BoW counts). In `scikit-learn`, this is done by a type of transformer called a `CountVectorizer`. We also must  choose whether to exclude uncommon words (i.e., words that only appear in a few documents) or very common words (\"stopwords\"). These high-level settings as a whole are called **hyperparameters** (different from **parameters**, which are the values learned by the model from the training data that enable it to make predictions).\n",
    "3. **Train.** You'll need to choose which learning model/algorithm to use, either by reading the documentation or talking to your friendly neighborhood data scientist. After choosing a model, we train it on part of the labeled input data (the training set).\n",
    "4. **Test.** We then use the trained model to predict/infer the labels of the remainder of the labeled input data (the test/validation set).\n",
    "5. **Assess.** Apply one or more metrics (scores) to evaluate how well the predicted labels match the actual labels of the test/validation set. If the performance is unsatisfactory, we'll need to backtrack, possibly all the way to step #1, getting more labeled data and applying different transformations/hyper-parameters as needed, and/or trying a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/csbaile3/opt/anaconda3/envs/jumpstart/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cjCVDXe05aY"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6a1df9f4b4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkRa6ylt2mf5"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Libraries are cool places.\",\n",
    "    \"Librarians can be cool too.\",\n",
    "    \"Digital libraries provide lots of resources.\",\n",
    "    \"Are digital libraries places?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGZ9CnLb5JSL",
    "outputId": "5c8daf17-062a-476a-803d-53b86ae1f2e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(\"Vocabulary size: \", len(count_vectorizer.vocabulary_))\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYbbsaSw5JSP",
    "outputId": "92ec9a65-f6ab-49a4-aaa7-47d166fcf1d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = count_vectorizer.transform(documents)\n",
    "print(counts)\n",
    "print(\"   ^  ^         ^\\n   |  |         |\\n  doc word_id count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1cQbsZo5JSS",
    "outputId": "3c65ecf9-b08b-4d6e-a11d-e4f0daa524fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will go through the entire vocabulary, but only show counts from the first doc\n",
    "doc = 0\n",
    "for word, word_id in count_vectorizer.vocabulary_.items():\n",
    "    print(word, \":\", counts[doc, word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLQ6FVJB5JSU",
    "outputId": "fa2e0ebf-325c-4c99-f25a-f1eaf7517788",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We previously used both the fit and transform methods.\n",
    "# Vectorizers typically have a single fit_transform method we can use to do both\n",
    "# in one step.\n",
    "counts = count_vectorizer.fit_transform(documents)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KTbDG4A5JSX"
   },
   "source": [
    "`CountVectorizer` also has some options to disregard stopwords, count ngrams (multiple adjacent words) instead of single words, cap the maximum number of words in each bag, normalize spelling, or count terms within a frequency range. It is worth exploring the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QngVrcpSlPHt"
   },
   "source": [
    "### Our text corpus\n",
    "\n",
    "Our actual corpus is made up of two pieces: tweets and texts from the the [Brown corpus](https://www.nltk.org/book/ch02.html) included in the Natural Language Toolkit (`nltk`), which contains more than a million words of English from 500 texts, where each text is categorized into one of 15 genres. We'll specifically work with the `news` genre. Our goal is to create a classifier that can distinguish between a tweet and news based solely on textual contents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHSaZn7slPHt"
   },
   "source": [
    "We'll start by importing our tweets and breaking them down into a format that works with our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvpKOYYblPHu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlOYIWF1lPHv",
    "outputId": "344df3a1-a7d2-483d-d151-43ab9276d328"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"sample_tweets_do_not_share.csv\")\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUciJkVvlPHx",
    "outputId": "d8aa3c49-a22c-4bd9-c3b8-de2d31c6d365"
   },
   "outputs": [],
   "source": [
    "tweets_english = tweets_df[tweets_df.lang == \"en\"][\"text\"]\n",
    "tweets_english.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDNY79GKlPHz"
   },
   "source": [
    "Let's take a sample of roughly 4000 sentences that roughly equals our sentences from the news corpus we'll get below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMekl_rblPH0",
    "outputId": "8a362a65-908e-461c-cb9c-44158ec76fcb"
   },
   "outputs": [],
   "source": [
    "tweets_english_sample = tweets_english.sample(4000)\n",
    "tweets_english_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2No_nn1lPH1"
   },
   "outputs": [],
   "source": [
    "# Build list of text, label tuples\n",
    "tweet_corpus = [(text, \"tweet\") for text in tweets_english_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxLsvWYN5JSX",
    "outputId": "4883ecc0-f72f-4b6d-9d59-1c0f6e4abf75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g826pWNg5JSa",
    "outputId": "2736ca8d-da2e-428a-8368-63e3e98336c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for category in brown.categories():\n",
    "    print(category, len(brown.fileids(category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUrRzrFi5JSc",
    "outputId": "5fc076a5-a918-4402-cd75-770f2ddbde47"
   },
   "outputs": [],
   "source": [
    "news_corpus = []\n",
    "for fileid in brown.fileids(\"news\"):\n",
    "    text = \" \".join(brown.words(fileids=fileid))\n",
    "    news_corpus.append((text, \"news\"))\n",
    "\n",
    "len(news_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulB09M975JSe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Typically you do want to hoist imports to the top of a file rather than have them sprinkled throughout. That said, while experimenting and prototyping, we often don't know what we need until we need it, and we can always reformat later.\n",
    "from nltk import sent_tokenize\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "butlK8CflPH_",
    "outputId": "9d6bd790-bc16-42ad-95cf-08fdb3236b10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to download punkt, a specific sentence tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNC7kx4e5JSh"
   },
   "outputs": [],
   "source": [
    "# Function that takes a list of tuples, and returns a list of tuples where the first item in a tuple is a single sentence text fragment\n",
    "def sent_chunker(labelled_corpus):\n",
    "    label = labelled_corpus[0][1]\n",
    "    chunks = [text for text, label in labelled_corpus]\n",
    "    sents = [sent_tokenize(text) for text in chunks]\n",
    "    flat_sents = list(chain(*sents))\n",
    "    labelled_flat_sents = [(sent, label) for sent in flat_sents]\n",
    "    return labelled_flat_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHzkQdeO5JSj"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZITMABE55JSl",
    "outputId": "7d42d73f-20f8-4657-a2aa-4e14183af98b"
   },
   "outputs": [],
   "source": [
    "sentences = sent_chunker(news_corpus)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ba5bWUiSlPIG"
   },
   "source": [
    "To see if the texts in our two corpora are roughly equivalent in length, let's check the average length of the texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRyl2XV95JSo",
    "outputId": "46c8fde7-0ffd-4a07-ca65-fdaa842dde8b"
   },
   "outputs": [],
   "source": [
    "np.mean([len(sent) for (sent, _) in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJqsuou5lPII",
    "outputId": "a575a2c4-5fd9-449a-d998-92f98cc85614"
   },
   "outputs": [],
   "source": [
    "# Let's check the avg length of the tweets\n",
    "np.mean([len(tweet) for (tweet, _) in tweet_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sng6jfklPIK"
   },
   "source": [
    "We'll combine the two lists of tuples together to create a single corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlBIAJXIlPIK"
   },
   "outputs": [],
   "source": [
    "full_corpus = tweet_corpus + sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iy1dmydJlPIM"
   },
   "source": [
    "For `scikit-learn`, we need to separate the texts and the labels. We'll use a couple of list comprehensions for that.\n",
    "\n",
    "Usually, the variable containing the labels is named `y`, and the one containing the input features (in our case, the texts) is named `X`, as in you can obtain the output `y` as a function of the inputs `X`, which is the core abstraction in `scikit-learn`. But using arbitrary letters is confusing when you're trying to learn a new concept, so we'll add some explanatory info to the variable names after `X_` and `y_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqJQliD45JSq"
   },
   "outputs": [],
   "source": [
    "texts = [text for (text, _) in full_corpus]\n",
    "labels = [label for (_, label) in full_corpus]\n",
    "X_texts = np.array(texts)\n",
    "y_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1N7ufxglPIO"
   },
   "source": [
    "Before we can move forward, we need to take a brief conceptual detour and talk about the ways that we often split corpora for machine learning tasks. \n",
    "\n",
    "Many machine learning approaches call for splitting the labeled data into three sets:\n",
    "- **training** data (usually the largest set) for the initial model training\n",
    "- **validation** data, which is then used to evaluate the initial performance of the model and subsequently fine-tune the model settings and **hyperparameters** in the hopes of getting better results\n",
    "- **testing** data is \"held out\" until all model tuning is completed and then is used to give a final evaluation score or *benchmark* of the model's performance.\n",
    "\n",
    "`scikit-learn` provides  functions to split a labeled dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOFwxhQ5lPIP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kr6Im2BPlPIR",
    "outputId": "5ff1f9d8-ad04-403d-9a9e-cee7eb2f8d5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_texts_train, X_texts_test, y_labels_train, y_labels_test) = train_test_split(X_texts, y_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"{len(X_texts_train)} training documents\")\n",
    "print(f\"{len(X_texts_test)} testing documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ9aD56RlPIS",
    "outputId": "0266bda4-b92b-4427-e622-776250a115fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_features_train = vectorizer.fit_transform(X_texts_train)\n",
    "X_features_test = vectorizer.transform(X_texts_test)\n",
    "\n",
    "print(f\"{X_features_train.shape[0]} training documents with {X_features_train.shape[1]} features\")\n",
    "print(f\"{X_features_test.shape[0]} test documents with {X_features_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoQpdI00lPIU"
   },
   "source": [
    "### Classification (prediction)\n",
    "\n",
    "Let's start with one of the Naïve Bayes classifiers.\n",
    "\n",
    "**Naïve Bayes** is a family of classifiers based on Bayes' Theorem of probability, which describes the probability of an event based on prior knowledge of possibly relevant conditions. Although its formulation can get confusing, all the math boils down to counting, multiplication and division, making Naïve Bayes (NB) classifiers very fast. On the other hand, NB makes the assumption that all of the features in the data set are equally important and independent, which is obviously not true for words. Despite this, Naïve Bayes classifiers are generally very accurate as text classifiers.\n",
    "\n",
    "<div align=\"left\"><b>\"All models are wrong but some are useful\" - George Box (1978)</b></div>\n",
    "\n",
    "There are three Naïve Bayes algorithms in `scikit-learn`: \n",
    "- Gaussian: assumes that features follow a normal distribution.\n",
    "- Multinomial: good for discrete counts, like in text classification problems using counts of words.\n",
    "- Bernoulli: useful for feature vectors that are binary (i.e. zeros and ones), like classic bag of words.\n",
    "\n",
    "Given that our feature vectors are counts of the words in each document with some additional vocabulary constraints, we will use `MultinomialNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JW8FkueBlPIU"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBspbJXslPIW",
    "outputId": "efb2de92-b509-4a9b-804c-6abb06b20145"
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_features_train, y_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXts3PIQlPIY",
    "outputId": "90fcca6c-5b86-42a3-cf79-29beb9738c51"
   },
   "outputs": [],
   "source": [
    "np.shape(classifier.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZszX9GelPIa"
   },
   "source": [
    "Now we can predict the categories of previously unseen texts and assess how good our classifier is at classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1S37nujlPIa",
    "outputId": "9c9cb1f0-e66a-4c08-bff1-ebb9bfe5cdf2"
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"The city's hospitals celebrated a grand opening on Tuesday.\",\n",
    "    \"This meme is lit.\",\n",
    "    \"The forest fire burned for 12 days before firefighters could extinguish it.\",\n",
    "    \"The disease spread quickly through Brazil.\",\n",
    "    \"@jack needs to get back to verifying folks #bluecheck\"\n",
    "]\n",
    "\n",
    "transformed_samples = vectorizer.transform(samples)\n",
    "classifier.predict(transformed_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AlqtNJBlPIc"
   },
   "source": [
    "How could we start to understand why the model makes the predictions that it does? \n",
    "\n",
    "One of the easiest things to do is to use a package that breaks down the prediction probabilities and which features led to those predictions. \n",
    "\n",
    "There are a number of options out there, but we'll use Lime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EY5IKVwzlPIc",
    "outputId": "77ed28c7-8e15-4197-b9c6-b7486f418a94",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If it isn't already in your environment.\n",
    "!pip install Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmvpKVwslPIe"
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bqjvMJClPIg",
    "outputId": "38a7b3ff-7259-43cf-8559-7edc1952707c"
   },
   "outputs": [],
   "source": [
    "def explain(entry, clf, vectorizer=None, n=10):\n",
    "    if vectorizer is None:\n",
    "        class_names = clf.steps[1].classes_.tolist()\n",
    "        pipeline = clf\n",
    "    else:\n",
    "        class_names = clf.classes_.tolist()\n",
    "        pipeline = make_pipeline(vectorizer, clf)\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(entry, pipeline.predict_proba, num_features=n)\n",
    "    exp.show_in_notebook()\n",
    "\n",
    "explain(\"Reports indicated that the cave entrance collapsed, trapping them inside.\", classifier, vectorizer)\n",
    "#explain(\"They went to a beautiful restaurant, and drank wine together.\", classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGOGVWX4lPIi"
   },
   "source": [
    "We can extract some of this information from the model ourselves. If we wanted to know which words are being used to decide whether a text is either `news` or `tweet`, we need to take into account the the word vocabulary built by the vectorizer we used to transform our data, and the feature (word) probabilities calculated for each label by the model at training time.\n",
    "\n",
    "The following is a fairly standard most informative features function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE1bGlUXlPIi",
    "outputId": "beddcdc4-9017-45f5-970f-1748d76c6409",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_informative_features(classifier, vectorizer=None, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    if vectorizer is None:\n",
    "        feature_names = classifier.steps[0].get_feature_names()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_log_prob_[0], feature_names))[-n:]\n",
    "    topn_class2 = sorted(zip(classifier.feature_log_prob_[1], feature_names))[-n:]\n",
    "    for prob, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], prob, feat)\n",
    "    print()\n",
    "    for prob, feat in reversed(topn_class1):\n",
    "        print(class_labels[0], prob, feat)\n",
    "\n",
    "most_informative_features(classifier, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAgGu6ULlPIk"
   },
   "source": [
    "You'll notice that a lot of the words here are commonly occuring words. We could decide that these are \"stopwords\" and remove them from the corpus in a preprocessing step. I tend to do first runs in an ml task without much preprocessing when possible, in order to get a baseline for the model and understand the corpus a bit better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hdsv_LqlPIk"
   },
   "source": [
    "### Model Evaluation and Selection\n",
    "\n",
    "Changing model hyper-parameters without really having a way to assess its performance can get problematic. If for example you find better informative features but the classifier is failing 50% of the time, it doesn't really matter much how good you think those features are. In some cases, very performant classifiers make use of unexpected or counterintuitive features.\n",
    "\n",
    "The only real way to assess its performance is by making the classifier predict labels for unseen data, and then comparing the predicted labels with the real labels. This is where the importance of separating training and testing data lies in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7MIBXmclPIm",
    "outputId": "6db24b5d-a018-433e-d037-ac4b837d8bea"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_features_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdrKe8hBlPIo",
    "outputId": "5448be90-0370-42aa-bf39-9358d90cc0d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Label     Predicted    Result\\n-----     ---------    ------\")\n",
    "for i, real_label in enumerate(y_labels_test[:20]):\n",
    "    predicted_label = y_pred[i]\n",
    "    if real_label == predicted_label:\n",
    "        result =  \"hit\"\n",
    "    else:\n",
    "        result = \"miss\"\n",
    "    print(real_label, \"    \", predicted_label, \"       \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jEVRUFQlPIq"
   },
   "source": [
    "We have almost entirely hits here because of how different the types of text within our corpus are. If our texts had more overlap in their content, it would harder for the classifier to work so effectively. \n",
    "\n",
    "This basic test gives us a sense of what's going on, but doesn't say too much about what types of fails we have. Were the tweets being classified as news or the other way around? This may not be too significant here, but if we think about using ML for medical diagnosis, it's a lot more important to understand if your model is giving you false positives or false negatives. \n",
    "\n",
    "One way of gaining this information is to build a confusion matrix. You could do this manually, but scikit-learn does contain a convenience function. \n",
    "\n",
    "![Confusion Matrix c/o Towards Data Science](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vBUyrMblPIq",
    "outputId": "a799ea68-1a4b-4c0f-9b53-b23685fcb515",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TN = true news\n",
    "# FN = false news\n",
    "# FT = false tweet\n",
    "# TT = true tweet\n",
    "\n",
    "print(\"TN FN\")\n",
    "print(confusion_matrix(y_labels_test, y_pred))\n",
    "print(\"FT TT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCkRAX72lPIs"
   },
   "source": [
    "We can see that when our classifier misses, it thinks that pieces of news are actually tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j484gh5HlPIt"
   },
   "source": [
    "Let's look quickly at one other common evaluation before moving on. **Accuracy** is ratio of correct predictions (\"hits\") to the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKrqoG-VlPIt",
    "outputId": "87dbe69e-abc2-44cc-c607-22677dd8913a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_labels_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzS27Ea8lPIv"
   },
   "source": [
    "Two other common metrics are *precision* and *recall*. They can be calculated (averaged) for the whole model or for each category. We'll think about the latter here.\n",
    "\n",
    "**Precision**: out of the test texts the model classified as tweets, what fraction of them were actually tweets?\n",
    "\n",
    "**Recall**: out of the total number of texts in the test set, what fraction of them did the model correctly classify as tweets?\n",
    "\n",
    "We aren't going to run these right now, but if you'd like to practice exploring the scikit-learn docs, you might try to find the correct functions and use them with our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X92F_gBI0Ujc"
   },
   "source": [
    "## Library applications of machine learning\n",
    "\n",
    "There are a large number of potential applications of machine learning in the library world, including working with collections and metadata, improving administrative workflows, and provisioning data services. We're only going to focus on one application today, which is created metadata for digitized collections through machine learning. There are a lot of different reports on data science and machine learning in libraries out there, but if you're looking for follow up reading, we specifically recommend these:\n",
    "\n",
    "- [Responsible Operations: Data Science, Machine Learning, and AI in Libraries](https://www.oclc.org/research/publications/2019/oclcresearch-responsible-operations-data-science-machine-learning-ai.html) by Thomas Padilla\n",
    "- [Machine Learning + Libraries: A Report on the State of the Field](https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf?loclr=blogsig) from LC Labs and written by Ryan Cordell. \n",
    "- [Mapping the Current Landscape of Research Library Engagement with Emerging Technologies in Research and Learning](https://www.arl.org/resources/mapping-the-current-landscape-of-research-library-engagement-with-emerging-technologies-in-research-and-learning/) from ARL, CNI, and Educause. This isn't specifically and only about ML in libraries, but it's a heavy and recurring theme. \n",
    "- [Shifting to Data Savvy: The Future of Data Science in Libraries](http://d-scholarship.pitt.edu/33891/1/Shifting%20to%20Data%20Savvy.pdf), an IMLS funded report on librarians and data science. \n",
    "\n",
    "None of these should be taken as exhaustive, and I think that all are at least somewhat controversial, but they are good starts to understanding potential futures of machine learning (and data science) in libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at metadata creation, we're going to work with a collection of digitized materials from NC State Libraries' Animal Turn collection. We're going to try to generate some meaningful metadata from both the text of the items and the page images.\n",
    "\n",
    "The overview from special collections is [here](https://www.lib.ncsu.edu/animal-turn).\n",
    "\n",
    "The digitized materials are [here](https://d.lib.ncsu.edu/collections/catalog?_=1543444883344&f%5Bispartof_facet%5D%5B%5D=Animal+Turn).\n",
    "\n",
    "We've taken the liberty of writing some scraping code that uses the IIIF manifests that back the collection to retrieve the OCR text and page images of each item.  Due to time, we won't be able to walk through all of the code to produce these models or run them during this workshop. Instead we're going to load the models we've built and talk through what we might gain from using these models and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start just by looking at a couple of texts to see what we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = glob.glob(\"texts/*.txt\")\n",
    "fns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open(fns[0], 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with using topic modeling to build a semantic model of the corpus. Topic modeling refers to a set of unsupervised machine learning algorithms and approaches to modeling the underlying thematic structure of a corpus of texts. The assumption is that the corpus is composed from a range of themes, and each text can be broken down to being some percentage about each theme. Each theme, or topic, is an aggregation of regularly co-occuring words. \n",
    "\n",
    "For example, in a corpus about animal rights, a single text or document might be 75% about household pets, 15% about training strategies, 5% about food, and 5% about cleaning. Another document might be 60% about farm animals, 25% about vetrinary care, and 15% about food. These breakdowns are determined by word occurence and co-occurence in relation to the topics the model constructed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've built a model that assumes there are roughly 40 topics or themes in text of our digitized Animal Turn collection. I've done a bit of preprocessing with the corpus, removing stopwords, numbers, and non-letter characters. I've also removed single letter words that weren't caught by the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel.load(\"animalturn_40_full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's useful, but it can be hard to keep think of this as an overview. Let's switch to visualizing the model using the `pyLDAvis` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `pyLDAvis` to work, we do need to load in data constructed during the creation of the model, namely the processed `gensim` corpus and the dictionary of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_corpus = gensim.corpora.MmCorpus(\"items_bow_lg_full.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.dictionary.Dictionary.load(\"animalturn_40_full.model.id2word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(model, loaded_corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better way to explore the topics, and to get some sense of how the topics might related to each other. A librarian could use this sort of visualization to help explore and describe large digitized collections and how patrons could engage them. \n",
    "\n",
    "What this view doesn't let us explore is how the individual documents in the collection are modeled according to topics, or how they might related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at a single document's significant topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_document_topics(loaded_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an effective, but slow way to understand individual documents. What if were interested in the groupings of documents based on our topic model? Based on the topic distributions for each document, I fit the corpus into 2d space using t-distributed stochastic neighbor embedding (tSNE), a dimensionality reduction algorithm for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"animal_turn_tsne_40.html\" height=\"800\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9di67C40S_G"
   },
   "outputs": [],
   "source": [
    "- Make sure we have both text and image examples. \n",
    "- Examples could be aspirational, not just things we definitely already know how to do. \n",
    "- Maybe use Animal Turn for both text and images; see if you can identify when an image is a photograph vs a cartoon or illustration. That could be useful metadata to help someone access the collection. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "jumpstart_intro_ml_draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
